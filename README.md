
#  <a href="https://www.kaggle.com/code/anchsemen/gan-hw">**Homework_2 Имплементация GAN**</a>


### **Используемый датасет:** 
Для данной работы использовался датасет <a href="https://www.kaggle.com/datasets/jessicali9530/celeba-dataset">Celeba</a>, в котором находится свыше 200 тысяч изображений знаменитостей.

### **Цель эксперимента:** 
Имплементация и попытка достичь сходимости GAN. 

### **Идея эксперимента:**
Применить новую архитектуру генератора, заменяя сверточные слои на CSPup блоки, использовать дискриминатор как у DCGAN, и проверить сходимость сети на датасете CelebA, для которого уже подтверждена сходимость DCGAN, путем исследования различных гипотез, такие как применение регуляризации, изменение архитектуры сети, и т.п.

**Описание CSPupBlock:** CSPupBlock представляет собой модификацию CSP block из архитектуры Scaled YOLOv4. В CSPupBlock происходит следующее:

<img src=CSPupBlock.png>

При обучении GAN без проверки гипотез, получается следующий результат. Как видно по графику loss функций ни у генератора ни у дискриминатора не получается достичь сходимости за 5 эпох. И в целом, loss дискриминатора не максимизируется, но loss генератора со скачками минимизируется, но этого не достаточно. Получается что-то минимально похожее на лица, но это не тот результат, который хочется получить.

<img src=D_loss&G_loss_epochs5.png>
<img src=results_epochs5.png>

Со всеми графиками loss функций можно ознакомиться <a href="https://wandb.ai/anch-semen/HW2_GENmodels?nw=nwuseranchsemen">здесь</a>.

### **0 гипотеза**:
Не пробовать изменять архитектуру GAN, а увеличить количество эпох обучения в базовой архитектуре до 20. 

**Результаты 0 гипотезы:** Результаты лучше, чем на 5 эпохах (лица более выражены), но все равно видно по графикам loфss функций, что хорошей сходимости не наблюдается.  

<img src=D_loss&G_loss_hyp0epochs20.png>
<img src=results_hyp0epochs20.png>

### **1 гипотеза**:
Как было написано в <a href="https://arxiv.org/ftp/arxiv/papers/2006/2006.05132.pdf">статье</a>, воспользоваться:
1. Использовать Batch Normalization
2. Вместо активации ReLU применить Leaky-ReLU

**Результаты 1 гипотезы:** При сравнении результатов работы GAN на протяжении 5 эпох без применения улучшений и с использованием методов Batch Normalization и Leaky-ReLU, мы замечаем некоторое улучшение в качестве результатов. Однако, все еще отсутствует явная сходимость генератора и дискриминатора. Поэтому переходим

<img src=D_loss&G_loss_hyp1epochs5.png>
<img src=results_hyp1epochs5.png>

### **2 гипотеза**:
1. Увеличим в 2 раза batch_size
2. Применим L2-регуляризацию к генератору и дискриминатору

**Результаты 2 гипотезы:** В данном случае мы наблюдаем mode collapse в работе генератора, что негативно сказывается на обучении всей модели GAN. Этот режим характеризуется тем, что генератор производит только ограниченное разнообразие изображений, не способное отобразить полный спектр данных обучения. Такое поведение генератора отрицательно влияет на эффективность обучения модели, приводя к плохим результатам.

<img src=D_loss&G_loss_hyp2epochs5.png>
<img src=results_hyp2epochs5.png>

### **3 гипотеза**: 
1.label smoothing
2. lr Disc = 0.0003 lr Gen = 0.0008
3. Применим вместо сигмоиды гиперболический тангенс на последнем слое генератора

**Результаты 3 гипотезы:** Возник mode collapse в конце обучения, но видно как хорошо сходятся loss'ы генератора и дискриминатора. Данная архитектура показывает хорошую генерацию лиц. 

<img src=D_loss&G_loss_hyp3epochs5.png>
<img src=results_hyp3epochs5.png>

**Выводы:** Архитектура, описанная в третьей гипотезе, показала наилучшие результаты среди всех рассмотренных. Хотя наблюдался некоторый mode collapse, который приводит к ограниченному разнообразию генерируемых изображений, думаю что при дальнейшем улучшении данной архитектуры можно добиться значительного прогресса.
